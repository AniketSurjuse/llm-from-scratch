{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8423e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477523ab",
   "metadata": {},
   "source": [
    "## Simple implementation of Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83df12c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self,d_in, d_out, context_length, dropout, qkb_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias= qkb_bias)\n",
    "        self.W_key = nn.Linear(d_in,d_out,bias=qkb_bias)\n",
    "        self.W_value = nn.Linear(d_in,d_out,bias=qkb_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        b, n_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1,2)\n",
    "\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:n_tokens, :n_tokens],\n",
    "            -torch.inf\n",
    "        )\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores/self.d_out**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights = attn_weights @ values\n",
    "        return context_vec\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e38a9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self,d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalSelfAttention(d_in, d_out, context_length,dropout, qkv_bias )\n",
    "            for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out*num_heads, d_out*num_heads)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        context_vec = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.out_proj(context_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d25e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m mha = MultiHeadAttentionWrapper(d_in, d_out, max_length,\u001b[32m0.0\u001b[39m,num_heads )\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# batch = input_embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m context_vecs = mha(\u001b[43mbatch\u001b[49m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(context_vecs.shape)\n",
      "\u001b[31mNameError\u001b[39m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "d_in = 256\n",
    "max_length = 1024\n",
    "num_heads = 2\n",
    "d_out = d_in //num_heads\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, max_length,0.0,num_heads )\n",
    "\n",
    "#from data loader\n",
    "batch = input_embeddings\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad8a31",
   "metadata": {},
   "source": [
    "## Alternate Implementation - Parallel and Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ac5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias= False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out% num_heads == 0, 'd_out must be divisible by num_heads'\n",
    "\n",
    "        #this d_out is larger than the above implementation, it's actually d_out*num_heads of old implementation\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.head_dim = d_out//  num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        #view them to add extra dimension\n",
    "        keys = keys.view(b,n_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b,n_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b,n_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        #transpose to get (b, num_heads, n_tokens, head_dim)\n",
    "\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        #attn_score\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "\n",
    "        #masked attn_score\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:n_tokens, :n_tokens],\n",
    "            -torch.inf\n",
    "        )\n",
    "\n",
    "        #attn_weight\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores/ keys.shape[-1]**0.5 , dim = -1\n",
    "        )\n",
    "\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vecs = attn_weights @ values\n",
    "\n",
    "        context_vecs = context_vecs.transpose(1,2)\n",
    "\n",
    "        context_vecs = context_vecs.contiguous().view(b, n_tokens, self.d_out)\n",
    "\n",
    "        context_vecs = self.out_proj(context_vecs)\n",
    "\n",
    "        return context_vecs\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ecf60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
